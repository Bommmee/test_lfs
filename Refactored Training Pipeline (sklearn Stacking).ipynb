{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d588db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:\\Users\\Taeyoung\\4차 프로젝트 정리\\병합_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taeyoung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test metrics (per target) ===\n",
      "   target        R2      RMSE       MSE       MAE     NRMSE\n",
      "0    Y_01  0.056885  0.343903  0.118269  0.264795  0.253693\n",
      "1    Y_02  0.053110  0.375672  0.141130  0.294262  0.355926\n",
      "2    Y_03  0.041098  0.354065  0.125362  0.277102  0.349172\n",
      "3    Y_04  0.087038  2.538625  6.444617  2.074254  0.186500\n",
      "4    Y_05  0.052395  2.474994  6.125593  1.971637  0.079150\n",
      "5    Y_06  0.177758  1.589225  2.525636  0.712783  0.095619\n",
      "6    Y_07  0.050976  0.399141  0.159314  0.319735  0.126336\n",
      "7    Y_08  0.108163  0.625372  0.391091  0.484094  0.023777\n",
      "8    Y_09  0.099702  0.619394  0.383649  0.479565  0.023540\n",
      "9    Y_10  0.165629  0.830864  0.690335  0.619023  0.037089\n",
      "10   Y_11  0.067360  0.794418  0.631101  0.622519  0.032660\n",
      "11   Y_12  0.103093  0.624721  0.390276  0.483585  0.023807\n",
      "12   Y_13  0.102088  0.622645  0.387686  0.481791  0.023731\n",
      "13   Y_14  0.104881  0.621420  0.386163  0.481751  0.023672\n",
      "Saved pipeline to: C:\\Users\\Taeyoung\\4차 프로젝트 정리\\prediction_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except Exception:\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "import cloudpickle\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_FOLDS_META = 10\n",
    "ROOT_DIR = Path.home() / \"4차 프로젝트 정리\"\n",
    "DATA_PATH = ROOT_DIR\n",
    "TRAIN_PATH = ROOT_DIR / \"merged_train.csv\"\n",
    "PRED_PIPE_PATH = ROOT_DIR / \"prediction_pipeline.pkl\"\n",
    "\n",
    "# Optuna-best (from user's previous tuning)\n",
    "BEST_PARAMS = {\n",
    "    'CatBoostRegressor_tune': {\n",
    "        0: {\"depth\": 8, \"iterations\": 600, \"learning_rate\": 0.03, \"l2_leaf_reg\": 1.0, \"subsample\": 1.0},\n",
    "        1: {\"depth\": 10, \"iterations\": 600, \"learning_rate\": 0.03, \"l2_leaf_reg\": 5.0, \"subsample\": 1.0},\n",
    "        2: {\"depth\": 8, \"iterations\": 300, \"learning_rate\": 0.05, \"l2_leaf_reg\": 5.0, \"subsample\": 0.7},\n",
    "        3: {\"depth\": 10, \"iterations\": 600, \"learning_rate\": 0.03, \"l2_leaf_reg\": 5.0, \"subsample\": 0.7},\n",
    "        4: {\"depth\": 10, \"iterations\": 600, \"learning_rate\": 0.03, \"l2_leaf_reg\": 5.0, \"subsample\": 1.0},\n",
    "        5: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.05, \"l2_leaf_reg\": 5.0, \"subsample\": 0.9},\n",
    "        6: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.03, \"l2_leaf_reg\": 5.0, \"subsample\": 0.9},\n",
    "        7: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.03, \"l2_leaf_reg\": 3.0, \"subsample\": 0.9},\n",
    "        8: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.03, \"l2_leaf_reg\": 3.0, \"subsample\": 0.9},\n",
    "        9: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.05, \"l2_leaf_reg\": 1.0, \"subsample\": 1.0},\n",
    "        10: {\"depth\": 10, \"iterations\": 900, \"learning_rate\": 0.03, \"l2_leaf_reg\": 1.0, \"subsample\": 0.9},\n",
    "        11: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.05, \"l2_leaf_reg\": 5.0, \"subsample\": 0.7},\n",
    "        12: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.03, \"l2_leaf_reg\": 5.0, \"subsample\": 0.9},\n",
    "        13: {\"depth\": 8, \"iterations\": 900, \"learning_rate\": 0.03, \"l2_leaf_reg\": 5.0, \"subsample\": 0.9},\n",
    "    },\n",
    "    'HistGradientBoostingRegressor_tune': {\n",
    "        0: {\"learning_rate\": 0.05, \"max_depth\": None},\n",
    "        1: {\"learning_rate\": 0.03, \"max_depth\": None},\n",
    "        2: {\"learning_rate\": 0.05, \"max_depth\": 6},\n",
    "        3: {\"learning_rate\": 0.1, \"max_depth\": None},\n",
    "        4: {\"learning_rate\": 0.05, \"max_depth\": None},\n",
    "        5: {\"learning_rate\": 0.1, \"max_depth\": None},\n",
    "        6: {\"learning_rate\": 0.1, \"max_depth\": 8},\n",
    "        7: {\"learning_rate\": 0.05, \"max_depth\": 8},\n",
    "        8: {\"learning_rate\": 0.05, \"max_depth\": None},\n",
    "        9: {\"learning_rate\": 0.1, \"max_depth\": None},\n",
    "        10: {\"learning_rate\": 0.05, \"max_depth\": 8},\n",
    "        11: {\"learning_rate\": 0.1, \"max_depth\": 6},\n",
    "        12: {\"learning_rate\": 0.05, \"max_depth\": 6},\n",
    "        13: {\"learning_rate\": 0.1, \"max_depth\": 6},\n",
    "    },\n",
    "    'LGBMRegressor_tune': {\n",
    "        0: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 31, \"max_depth\": -1, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        1: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 31, \"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.7},\n",
    "        2: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 31, \"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.7},\n",
    "        3: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 8, \"subsample\": 1.0, \"colsample_bytree\": 0.7},\n",
    "        4: {\"n_estimators\": 400, \"learning_rate\": 0.05, \"num_leaves\": 31, \"max_depth\": -1, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        5: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 12, \"subsample\": 0.8, \"colsample_bytree\": 0.9},\n",
    "        6: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 31, \"max_depth\": -1, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        7: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 8, \"subsample\": 1.0, \"colsample_bytree\": 0.7},\n",
    "        8: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 8, \"subsample\": 1.0, \"colsample_bytree\": 0.7},\n",
    "        9: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": -1, \"subsample\": 0.7, \"colsample_bytree\": 1.0},\n",
    "        10: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 8, \"subsample\": 1.0, \"colsample_bytree\": 0.7},\n",
    "        11: {\"n_estimators\": 800, \"learning_rate\": 0.03, \"num_leaves\": 31, \"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.7},\n",
    "        12: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 8, \"subsample\": 1.0, \"colsample_bytree\": 0.7},\n",
    "        13: {\"n_estimators\": 400, \"learning_rate\": 0.03, \"num_leaves\": 127, \"max_depth\": 8, \"subsample\": 1.0, \"colsample_bytree\": 0.7},\n",
    "    },\n",
    "    'XGBRegressor_tune': {\n",
    "        0: {\"n_estimators\": 300, \"max_depth\": 4, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        1: {\"n_estimators\": 300, \"max_depth\": 4, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        2: {\"n_estimators\": 300, \"max_depth\": 4, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        3: {\"n_estimators\": 300, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 1.0},\n",
    "        4: {\"n_estimators\": 500, \"max_depth\": 4, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        5: {\"n_estimators\": 800, \"max_depth\": 6, \"learning_rate\": 0.03, \"subsample\": 1.0, \"colsample_bytree\": 1.0},\n",
    "        6: {\"n_estimators\": 500, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        7: {\"n_estimators\": 300, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.8, \"colsample_bytree\": 1.0},\n",
    "        8: {\"n_estimators\": 300, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.8, \"colsample_bytree\": 1.0},\n",
    "        9: {\"n_estimators\": 500, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.7},\n",
    "        10: {\"n_estimators\": 300, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.8, \"colsample_bytree\": 1.0},\n",
    "        11: {\"n_estimators\": 300, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.8, \"colsample_bytree\": 1.0},\n",
    "        12: {\"n_estimators\": 300, \"max_depth\": 6, \"learning_rate\": 0.03, \"subsample\": 0.8, \"colsample_bytree\": 1.0},\n",
    "        13: {\"n_estimators\": 300, \"max_depth\": 8, \"learning_rate\": 0.03, \"subsample\": 0.7, \"colsample_bytree\": 0.8},\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load data & split\n",
    "# -----------------------------\n",
    "print(\"Loading:\", TRAIN_PATH)\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "X_full = train.filter(regex='^X_').copy()\n",
    "Y_full = train.filter(regex='^Y_').copy()\n",
    "\n",
    "# drop columns (as in original training)\n",
    "drop_cols = ['X_04','X_23','X_47','X_48','X_10','X_11','X_02']\n",
    "X_full.drop(columns=[c for c in drop_cols if c in X_full.columns], inplace=True, errors='ignore')\n",
    "\n",
    "# (Optional) train-time pruning by X_33>6 — we keep it ONLY for training data\n",
    "if 'X_33' in X_full.columns:\n",
    "    drop_idx = X_full.index[X_full['X_33'] > 6]\n",
    "    if len(drop_idx)>0:\n",
    "        X_full = X_full.drop(index=drop_idx)\n",
    "        Y_full = Y_full.drop(index=drop_idx)\n",
    "\n",
    "X_full = X_full.reset_index(drop=True)\n",
    "Y_full = Y_full.reset_index(drop=True)\n",
    "\n",
    "X_tr_df, X_te_df, Y_tr_df, Y_te_df = train_test_split(\n",
    "    X_full, Y_full, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Impute + PCA (pure sklearn)\n",
    "# -----------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_tr_imp = pd.DataFrame(imputer.fit_transform(X_tr_df), columns=X_tr_df.columns, index=X_tr_df.index)\n",
    "X_te_imp = pd.DataFrame(imputer.transform(X_te_df), columns=X_te_df.columns, index=X_te_df.index)\n",
    "\n",
    "# same PCA grouping as original\n",
    "PCA_GROUPS = [\n",
    "    (\"g0\", ['X_13','X_14','X_15','X_16','X_17','X_18'], 5),\n",
    "    (\"g1\", ['X_19','X_20','X_21','X_22'], 2),\n",
    "    (\"g2\", ['X_34','X_35','X_36','X_37'], 1),\n",
    "    (\"g3\", ['X_41','X_42','X_43','X_44','X_45'], 1),\n",
    "    (\"g4\", ['X_50','X_51','X_52','X_53','X_54','X_55','X_56'], 2),\n",
    "]\n",
    "\n",
    "def apply_fit_pca(X_fit: pd.DataFrame, X_apply: pd.DataFrame):\n",
    "    pca_info = []\n",
    "    Xf = X_fit.copy(); Xa = X_apply.copy()\n",
    "    for name, cols, n_comp in PCA_GROUPS:\n",
    "        cols = [c for c in cols if c in Xf.columns]\n",
    "        if len(cols)==0:  # group not present\n",
    "            continue\n",
    "        pca = PCA(n_components=n_comp, random_state=SEED).fit(Xf[cols])\n",
    "        Zf = pca.transform(Xf[cols])\n",
    "        Za = pca.transform(Xa[cols])\n",
    "        comp_cols = [f\"PCA_{name}_{i}\" for i in range(n_comp)]\n",
    "        Xf = pd.concat([Xf.drop(columns=cols), pd.DataFrame(Zf, columns=comp_cols, index=Xf.index)], axis=1)\n",
    "        Xa = pd.concat([Xa.drop(columns=cols), pd.DataFrame(Za, columns=comp_cols, index=Xa.index)], axis=1)\n",
    "        pca_info.append({\n",
    "            \"group_name\": name,\n",
    "            \"group_cols\": cols,\n",
    "            \"n_components\": n_comp,\n",
    "            \"pca\": pca,\n",
    "            \"component_cols\": comp_cols,\n",
    "        })\n",
    "    return Xf, Xa, pca_info\n",
    "\n",
    "X_tr_final, X_te_final, pca_info = apply_fit_pca(X_tr_imp, X_te_imp)\n",
    "FINAL_FEATURE_NAMES = X_tr_final.columns.tolist()\n",
    "\n",
    "# numpy views\n",
    "X_tr = X_tr_final.to_numpy(); X_te = X_te_final.to_numpy()\n",
    "Y_tr = Y_tr_df.to_numpy();   Y_te = Y_te_df.to_numpy()\n",
    "\n",
    "y_cols = list(Y_full.columns)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build per-target StackingRegressor and train\n",
    "# -----------------------------\n",
    "models = {}\n",
    "cv_meta = KFold(n_splits=N_FOLDS_META, shuffle=True, random_state=SEED)\n",
    "\n",
    "for j, y_name in enumerate(y_cols):\n",
    "    est_list = []\n",
    "\n",
    "    # HistGBR\n",
    "    hgb_params = BEST_PARAMS['HistGradientBoostingRegressor_tune'].get(j, {\"random_state\": SEED})\n",
    "    hgb_params.setdefault('random_state', SEED)\n",
    "    est_list.append((\"hgb\", HistGradientBoostingRegressor(**hgb_params)))\n",
    "\n",
    "    # XGB\n",
    "    xgb_params = BEST_PARAMS['XGBRegressor_tune'].get(j, {\"random_state\": SEED, \"objective\": \"reg:squarederror\", \"tree_method\": \"hist\", \"n_jobs\": -1})\n",
    "    xgb_params.setdefault('random_state', SEED)\n",
    "    xgb_params.setdefault('objective', 'reg:squarederror')\n",
    "    xgb_params.setdefault('tree_method', 'hist')\n",
    "    xgb_params.setdefault('n_jobs', -1)\n",
    "    est_list.append((\"xgb\", XGBRegressor(**xgb_params)))\n",
    "\n",
    "    # LGBM\n",
    "    lgb_params = BEST_PARAMS['LGBMRegressor_tune'].get(j, {\"random_state\": SEED, \"verbose\": -1})\n",
    "    lgb_params.setdefault('random_state', SEED)\n",
    "    lgb_params.setdefault('verbose', -1)\n",
    "    est_list.append((\"lgb\", LGBMRegressor(**lgb_params)))\n",
    "\n",
    "    # CatBoost (optional)\n",
    "    if HAS_CATBOOST:\n",
    "        cat_params = BEST_PARAMS['CatBoostRegressor_tune'].get(j, {\"random_seed\": SEED, \"verbose\": False, \"allow_writing_files\": False, \"loss_function\": \"RMSE\"})\n",
    "        cat_params.setdefault('random_seed', SEED)\n",
    "        cat_params.setdefault('verbose', False)\n",
    "        cat_params.setdefault('allow_writing_files', False)\n",
    "        cat_params.setdefault('loss_function', 'RMSE')\n",
    "        est_list.append((\"cat\", CatBoostRegressor(**cat_params)))\n",
    "\n",
    "    stack = StackingRegressor(\n",
    "        estimators=est_list,\n",
    "        final_estimator=Ridge(random_state=SEED),\n",
    "        cv=cv_meta,\n",
    "        passthrough=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    stack.fit(X_tr, Y_tr[:, j])\n",
    "    models[y_name] = stack\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Evaluate (optional print)\n",
    "# -----------------------------\n",
    "rows = []\n",
    "for j, y_name in enumerate(y_cols):\n",
    "    y_true = Y_te[:, j]\n",
    "    y_pred = models[y_name].predict(X_te)\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    denom = np.mean(np.abs(y_true)) if np.mean(np.abs(y_true))!=0 else 1.0\n",
    "    nrmse = rmse / denom\n",
    "    rows.append([y_name, r2, rmse, mse, mae, nrmse])\n",
    "\n",
    "metrics_df = pd.DataFrame(rows, columns=['target','R2','RMSE','MSE','MAE','NRMSE'])\n",
    "print(\"\\n=== Test metrics (per target) ===\")\n",
    "print(metrics_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save artifacts with cloudpickle\n",
    "# -----------------------------\n",
    "artifacts = {\n",
    "    'imputer': imputer,\n",
    "    'pca_info': pca_info,\n",
    "    'pre_drop_cols': ['X_04','X_23','X_47','X_48','X_10','X_11','X_02'],\n",
    "    'final_feature_names': FINAL_FEATURE_NAMES,\n",
    "    'target_columns': y_cols,\n",
    "    'models': models,\n",
    "}\n",
    "\n",
    "PRED_PIPE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(PRED_PIPE_PATH, 'wb') as f:\n",
    "    cloudpickle.dump(artifacts, f)\n",
    "\n",
    "print(f\"Saved pipeline to: {PRED_PIPE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a1951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
